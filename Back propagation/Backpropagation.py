import numpy as np

np.random.seed(0)

# -----------------------------
# Sample Input and Target
# -----------------------------
X = np.array([[1, 1, 0, 1]])     # input
T = np.array([[1, 0]])           # target

lr = 0.5

# -----------------------------
# Sigmoid Functions
# -----------------------------
def sigmoid(x):
    return 1/(1+np.exp(-x))

def dsigmoid(y):
    return y*(1-y)

# -----------------------------
# Initialize Weights
# -----------------------------
W1 = np.random.randn(4,3)   # 4â†’3
W2 = np.random.randn(3,2)   # 3â†’2
W3 = np.random.randn(2,2)   # 2â†’2

b1 = np.zeros((1,3))
b2 = np.zeros((1,2))
b3 = np.zeros((1,2))

# =============================
# FORWARD PASS
# =============================

Z1 = X @ W1 + b1
A1 = sigmoid(Z1)

Z2 = A1 @ W2 + b2
A2 = sigmoid(Z2)

Z3 = A2 @ W3 + b3
Y  = sigmoid(Z3)

print("Hidden1 Output:", A1)
print("Hidden2 Output:", A2)
print("Network Output:", Y)

# =============================
# ERROR
# =============================

E = T - Y
print("\nOutput Error:", E)

# =============================
# BACKPROPAGATION
# =============================

d3 = E * dsigmoid(Y)
d2 = (d3 @ W3.T) * dsigmoid(A2)
d1 = (d2 @ W2.T) * dsigmoid(A1)

# =============================
# WEIGHT UPDATES
# =============================

W3 += A2.T @ d3 * lr
b3 += d3 * lr

W2 += A1.T @ d2 * lr
b2 += d2 * lr

W1 += X.T @ d1 * lr
b1 += d1 * lr

print("\nUpdated W3:\n", W3)
print("\nUpdated W2:\n", W2)
print("\nUpdated W1:\n", W1)





---------------------------------------------------------------------------------------------


Step 1 â€” Imports & Seed
import numpy as np
np.random.seed(0)

âœ… Meaning

numpy â†’ matrix math

random seed â†’ same weights every run (repeatable output for lab)

ğŸ”¹ Step 2 â€” Input & Target
X = np.array([[1, 1, 0, 1]])
T = np.array([[1, 0]])

âœ… Meaning

X = one training sample

T = desired output class label

Shape:

X â†’ (1 Ã— 4)
T â†’ (1 Ã— 2)

ğŸ”¹ Step 3 â€” Learning Rate
lr = 0.5

âœ… Meaning

Controls how much weights change during update.

Higher â†’ faster but risky
Lower â†’ slower but stable

ğŸ”¹ Step 4 â€” Sigmoid Functions
def sigmoid(x):
    return 1/(1+np.exp(-x))

def dsigmoid(y):
    return y*(1-y)

âœ… Meaning
Sigmoid activation

Converts net input â†’ value between 0 and 1.

Derivative

Needed for backprop gradient:

Ïƒâ€™ = y(1âˆ’y)


We pass output y directly â€” faster.

ğŸ”¹ Step 5 â€” Weight Initialization
W1 = np.random.randn(4,3)
W2 = np.random.randn(3,2)
W3 = np.random.randn(2,2)

âœ… Meaning

Weight matrices match layer sizes:

From	To	Shape
Input	Hidden1	4Ã—3
Hidden1	Hidden2	3Ã—2
Hidden2	Output	2Ã—2
Biases
b1 = zeros(1Ã—3)
b2 = zeros(1Ã—2)
b3 = zeros(1Ã—2)


Bias shifts neuron threshold.

=============================
ğŸš€ FORWARD PASS
=============================

This computes prediction.

ğŸ”¹ Hidden Layer 1
Z1 = X @ W1 + b1
A1 = sigmoid(Z1)

âœ… Meaning
Z1 = weighted sum
A1 = activated output


Shape:

A1 â†’ (1Ã—3)

ğŸ”¹ Hidden Layer 2
Z2 = A1 @ W2 + b2
A2 = sigmoid(Z2)


Output:

A2 â†’ (1Ã—2)

ğŸ”¹ Output Layer
Z3 = A2 @ W3 + b3
Y  = sigmoid(Z3)

âœ… Meaning

Final network prediction:

Y = predicted output (1Ã—2)


Printed as:

Hidden1 Output
Hidden2 Output
Network Output

=============================
âŒ ERROR
=============================
E = T - Y

âœ… Meaning

Difference between:

target â€“ prediction


This drives learning.

=============================
ğŸ” BACKPROPAGATION
=============================

This is the core algorithm.

We compute error signals (deltas) backward.

ğŸ”¹ Output Layer Delta
d3 = E * dsigmoid(Y)

Formula
delta_output = error Ã— sigmoid_derivative

ğŸ”¹ Hidden Layer 2 Delta
d2 = (d3 @ W3.T) * dsigmoid(A2)

Meaning

Error flows backward:

next_delta Ã— weights Ã— derivative


Chain rule applied.

ğŸ”¹ Hidden Layer 1 Delta
d1 = (d2 @ W2.T) * dsigmoid(A1)


Same idea â€” propagate further back.

=============================
ğŸ”§ WEIGHT UPDATES
=============================

Gradient descent step.

ğŸ”¹ Update Output Weights
W3 += A2.T @ d3 * lr
b3 += d3 * lr


Formula:

weight += inputáµ€ Ã— delta Ã— lr

ğŸ”¹ Update Hidden Weights

Same pattern:

W2 += A1.T @ d2 * lr
W1 += X.T  @ d1 * lr


Each layer uses:

previous layer output Ã— current delta

ğŸ“Š What You See Printed

The code prints:

âœ… Layer outputs
Hidden1 Output
Hidden2 Output
Network Output

âœ… Error vector
Output Error

âœ… Updated weights
W1, W2, W3 after learning


That proves backprop worked.
