import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from sklearn_extra.cluster import KMedoids

# -----------------------------
# Step 1: Load Iris Dataset
# -----------------------------
iris = load_iris()
X = iris.data
y_true = iris.target

print("Dataset shape:", X.shape)
print("Classes:", iris.target_names)

# -----------------------------
# Step 2: Standardize Features
# -----------------------------
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# -----------------------------
# Step 3: Apply K-Medoids
# -----------------------------
kmedoids = KMedoids(
    n_clusters=3,
    metric="euclidean",
    random_state=42
)

labels = kmedoids.fit_predict(X_scaled)

print("\nMedoid indices:", kmedoids.medoid_indices_)
print("\nMedoid points:\n", kmedoids.cluster_centers_)

# -----------------------------
# Step 4: Cluster Quality
# -----------------------------
score = silhouette_score(X_scaled, labels)
print("\nSilhouette Score:", round(score, 3))

# -----------------------------
# Step 5: PCA for Spatial Plot
# -----------------------------
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)
medoids_pca = pca.transform(kmedoids.cluster_centers_)

# -----------------------------
# Step 6: Plot Clusters + Medoids
# -----------------------------
plt.figure(figsize=(8,6))

for i in range(3):
    plt.scatter(
        X_pca[labels == i, 0],
        X_pca[labels == i, 1],
        label=f"Cluster {i+1}"
    )

# plot medoids clearly
plt.scatter(
    medoids_pca[:,0],
    medoids_pca[:,1],
    marker='X',
    s=300,
    label="Medoids"
)

plt.title("K-Medoids Clustering â€” Iris Dataset")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.legend()
plt.grid()




-------------------------------------------------------------------------




ğŸ”¹ Step 1 â€” Import Libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from sklearn_extra.cluster import KMedoids

âœ… Meaning

load_iris â†’ built-in Iris dataset

StandardScaler â†’ normalize features

KMedoids â†’ clustering algorithm

PCA â†’ reduce dimensions for plotting

silhouette_score â†’ cluster quality measure

matplotlib â†’ visualization

âš ï¸ KMedoids comes from sklearn-extra, not core sklearn.

ğŸ”¹ Step 2 â€” Load Dataset
iris = load_iris()
X = iris.data
y_true = iris.target

âœ… Meaning
X = 150 rows Ã— 4 features


Features:

sepal length

sepal width

petal length

petal width

y_true = real species labels (not used in clustering â€” only for reference)

ğŸ”¹ Step 3 â€” Standardize Features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

âœ… Why Needed

Clustering uses distance.

Features have different scales:

cm vs ratios vs counts


Standardization makes:

mean = 0
std = 1


So no feature dominates distance.

ğŸ”¹ Step 4 â€” Apply K-Medoids
kmedoids = KMedoids(
    n_clusters=3,
    metric="euclidean",
    random_state=42
)

âœ… Meaning

We configure the model:

n_clusters=3 â†’ Iris has 3 natural groups

metric â†’ distance type

random_state â†’ repeatable result

Train + Get Labels
labels = kmedoids.fit_predict(X_scaled)

âœ… Meaning

Algorithm:

1ï¸âƒ£ Pick initial medoids
2ï¸âƒ£ Assign points to nearest medoid
3ï¸âƒ£ Swap medoid candidates
4ï¸âƒ£ Minimize total distance
5ï¸âƒ£ Repeat until stable

Output:

labels â†’ cluster number for each sample

ğŸ”¹ Step 5 â€” Print Medoids
kmedoids.medoid_indices_
kmedoids.cluster_centers_

âœ… Meaning

Unlike K-Means:

center = mean (not real point)


K-Medoids:

center = actual data point


These are the chosen representative samples.

ğŸ”¹ Step 6 â€” Cluster Quality Score
score = silhouette_score(X_scaled, labels)

âœ… Meaning

Silhouette score measures:

how well points fit their cluster


Range:

-1 â†’ bad
0 â†’ overlap
+1 â†’ good separation


Higher = better clustering.

ğŸ”¹ Step 7 â€” PCA for Spatial Visualization
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

âœ… Why Needed

Data is 4-dimensional â†’ cannot plot directly.

PCA converts:

4D â†’ 2D


while preserving most variance.

This allows spatial plotting.

Transform Medoids Too
medoids_pca = pca.transform(kmedoids.cluster_centers_)


So medoids appear in same 2D space.

ğŸ”¹ Step 8 â€” Plot Clusters
for i in range(3):
    plt.scatter(X_pca[labels == i, 0],
                X_pca[labels == i, 1])

âœ… Meaning

For each cluster:

plot its points with same color


Creates visual grouping.

ğŸ”¹ Step 9 â€” Plot Medoids
plt.scatter(medoids_pca[:,0],
            medoids_pca[:,1],
            marker='X',
            s=300)

âœ… Meaning

Medoids are shown as:

big X markers
larger size
different symbol


So they stand out clearly.

This gives strong spatial understanding.

ğŸ”¹ Step 10 â€” Labels & Grid
plt.title(...)
plt.xlabel(...)
plt.ylabel(...)
plt.legend()
plt.grid()

âœ… Meaning

Makes plot readable and lab-presentable.

ğŸ¯ Final Output You See
ğŸ“Š Console
Medoid indices
Medoid values
Silhouette score

ğŸ“ˆ Plot
Colored clusters
Separated groups
Big X = medoids


This is the spatial output your question asked for.
plt.show()
