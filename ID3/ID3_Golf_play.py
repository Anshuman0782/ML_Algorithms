import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text
from sklearn.preprocessing import LabelEncoder

# -----------------------------
# Step 1: Golf Playing Dataset
# -----------------------------
data = {
    "Outlook": ["Sunny","Sunny","Overcast","Rain","Rain","Rain","Overcast",
                "Sunny","Sunny","Rain","Sunny","Overcast","Overcast","Rain"],

    "Temperature": ["Hot","Hot","Hot","Mild","Cool","Cool","Cool",
                    "Mild","Cool","Mild","Mild","Mild","Hot","Mild"],

    "Humidity": ["High","High","High","High","Normal","Normal","Normal",
                 "High","Normal","Normal","Normal","High","Normal","High"],

    "Wind": ["Weak","Strong","Weak","Weak","Weak","Strong","Strong",
             "Weak","Weak","Weak","Strong","Strong","Weak","Strong"],

    "Play": ["No","No","Yes","Yes","Yes","No","Yes",
             "No","Yes","Yes","Yes","Yes","Yes","No"]
}

df = pd.DataFrame(data)

print("Dataset:\n", df)

# -----------------------------
# Step 2: Encode Categorical Data
# -----------------------------
df_enc = df.copy()
encoders = {}

for col in df.columns:
    le = LabelEncoder()
    df_enc[col] = le.fit_transform(df[col])
    encoders[col] = le

# -----------------------------
# Step 3: Features and Target
# -----------------------------
X = df_enc.drop("Play", axis=1)
y = df_enc["Play"]

# -----------------------------
# Step 4: ID3 = Entropy Tree
# -----------------------------
model = DecisionTreeClassifier(
    criterion="entropy",   # ID3
    max_depth=4,
    random_state=0
)

model.fit(X, y)

# -----------------------------
# Step 5: Print Decision Rules
# -----------------------------
print("\nDecision Rules:\n")
print(export_text(model, feature_names=list(X.columns)))

# -----------------------------
# Step 6: Draw Decision Tree
# -----------------------------
plt.figure(figsize=(14, 9))

plot_tree(
    model,
    feature_names=X.columns,
    class_names=encoders["Play"].classes_,
    filled=True,
    rounded=True
)

plt.title("ID3 Decision Tree â€” Golf Playing Dataset")
plt.show()





Output:-
1. If Outlook = Overcast â†’ Play = Yes

2. If Outlook = Sunny AND Humidity = High â†’ Play = No

3. If Outlook = Rain AND Humidity = High AND Wind = Strong â†’ Play = No

4. If Outlook = Rain AND Humidity = High AND Wind = Weak â†’ Play = Yes

5. If Humidity = Normal AND Wind = Weak â†’ Play = Yes

6. If Humidity = Normal AND Wind = Strong AND Temperature = Mild â†’ Play = Yes

7. If Humidity = Normal AND Wind = Strong AND Temperature = Cool/Hot â†’ Play = No


-----------------------------------------------------------------------------------------------------

ðŸŒ³ What This Tree Represents

This is a Decision Tree built using ID3 (Entropy + Information Gain) to predict:

Play = Yes or No


based on:

Outlook, Temperature, Humidity, Wind


The tree shows:

ðŸ‘‰ which feature is checked first
ðŸ‘‰ what condition is tested
ðŸ‘‰ how data is split
ðŸ‘‰ final Yes/No decision

ðŸ“¦ How to Read Each Node Box

Each node shows something like:

Outlook <= 0.5
entropy = 0.94
samples = 14
value = [5, 9]
class = Yes


Letâ€™s decode each line.

ðŸ”¹ Condition Line (Top Line)

Example:

Outlook <= 0.5


Means:

ðŸ‘‰ Tree is splitting on Outlook feature
ðŸ‘‰ Because it gave highest information gain (ID3 rule)

Since data was label-encoded:

Example mapping might be:

Overcast = 0
Rain = 1
Sunny = 2


So:

Outlook <= 0.5 â†’ Overcast branch
Outlook > 0.5 â†’ Rain/Sunny branch

ðŸ”¹ Entropy

Example:

entropy = 0.94


Entropy measures impurity:

Entropy	Meaning
0	Pure (all Yes or all No)
1	Fully mixed

So:

0.94 â†’ mixed Yes/No
0.0 â†’ perfectly pure


Leaves with entropy = 0 are final decisions.

ðŸ”¹ Samples
samples = 14


Number of rows reaching that node.

Root node = all 14 golf records.

Child nodes = subset after split.

ðŸ”¹ Value
value = [5, 9]


Counts of each class:

[No, Yes]


So:

5 = No
9 = Yes

ðŸ”¹ Class
class = Yes


Majority class at that node.

Tree predicts this if it stops here.

ðŸŒ² Now Letâ€™s Read Your Tree Logically
ðŸŸ¦ Root Node
Outlook <= 0.5
entropy = 0.94
samples = 14
value = [5,9]
class = Yes


Meaning:

ðŸ‘‰ First split chosen = Outlook
ðŸ‘‰ Because ID3 found highest information gain here

ðŸŸ¦ Left Branch â€” Outlook = Overcast
entropy = 0.0
samples = 4
value = [0,4]
class = Yes


âœ… All are Yes
âœ… Pure node
âœ… Leaf node

Rule: If Outlook = Overcast â†’ Play = Yes

ðŸŸ§ Right Branch â€” Outlook â‰  Overcast

Next split:

Humidity <= 0.5


Means ID3 next best feature = Humidity.

ðŸŸ§ Humidity High Branch

Leads mostly to:

class = No


Rule:

If Outlook = Sunny AND Humidity = High â†’ No

ðŸŸ¦ Humidity Normal Branch

Next split:

Wind <= 0.5


Means wind decides here.

ðŸŸ¦ Wind Weak
class = Yes

ðŸŸ§ Wind Strong
class = No


Rule:

If Humidity = Normal AND Wind = Weak â†’ Yes
If Humidity = Normal AND Wind = Strong â†’ No

ðŸ“ Final Decision Rules (From Your Tree)

You can write this in exam:

1. If Outlook = Overcast â†’ Play = Yes
2. If Outlook = Sunny AND Humidity = High â†’ Play = No
3. If Outlook = Sunny AND Humidity = Normal â†’ Play = Yes
4. If Outlook = Rain AND Wind = Strong â†’ Play = No
5. If Outlook = Rain AND Wind = Weak â†’ Play = Yes
