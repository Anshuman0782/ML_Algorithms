import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# -----------------------------
# Step 1: Load Iris Dataset
# -----------------------------
iris = load_iris()
X = iris.data
y = iris.target

feature_names = iris.feature_names
class_names = iris.target_names

print("Feature Names:", feature_names)
print("Class Names:", class_names)

# -----------------------------
# Step 2: Train-Test Split
# -----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.25,
    random_state=42
)

# -----------------------------
# Step 3: ID3 = Decision Tree with Entropy
# -----------------------------
model = DecisionTreeClassifier(
    criterion="entropy",   # ID3
    max_depth=4,           # keeps tree readable
    random_state=42
)

model.fit(X_train, y_train)

# -----------------------------
# Step 4: Predictions
# -----------------------------
y_pred = model.predict(X_test)

print("\nAccuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# -----------------------------
# Step 5: Text Rules (Very Helpful Output)
# -----------------------------
print("\nDecision Tree Rules:\n")
print(export_text(model, feature_names=feature_names))

# -----------------------------
# Step 6: Draw Spatial Decision Tree
# -----------------------------
plt.figure(figsize=(18, 10))

plot_tree(
    model,
    feature_names=feature_names,
    class_names=class_names,
    filled=True,
    rounded=True,
    fontsize=11
)

plt.title("ID3 Decision Tree â€” Iris Dataset")
plt.show()





âœ… 2ï¸âƒ£ Accuracy = 1.0
Accuracy: 1.0

Meaning (simple)

Model predicted 100% test samples correctly

Accuracy formula:

correct predictions / total predictions


Here:

38 correct / 38 total = 1.0

âš ï¸ Viva Note

Iris dataset is very clean â†’ decision trees often get perfect accuracy.

This is normal, not cheating.

âœ… 3ï¸âƒ£ Classification Report

Example line:

precision recall f1-score support


Letâ€™s decode one row:

class 0 (setosa)
precision = 1.00
recall = 1.00
f1 = 1.00
support = 15

Meanings in easy words
ğŸ¯ Precision

When model predicts setosa â†’ how often correct
= 100%

ğŸ” Recall

Out of all real setosa â†’ how many found
= 100%

âš–ï¸ F1 Score

Balance of precision & recall
= Perfect

ğŸ“¦ Support

Number of test samples in that class

âœ… 4ï¸âƒ£ Decision Tree Rules â€” Most Important Part

This is your ID3 decision logic:

petal length <= 2.45 â†’ class 0

ğŸŒ¸ Rule 1 â€” Setosa Detection

If:

petal length â‰¤ 2.45


â†’ Always setosa

This is biologically true â€” setosa has very small petals.

So ID3 correctly chose petal length as root split
(because highest information gain).

ğŸŒ¸ Rule 2 â€” Versicolor vs Virginica Split

Next:

petal length â‰¤ 4.75
    petal width â‰¤ 1.65 â†’ versicolor
    else â†’ virginica


Meaning:

Medium petals â†’ check width:

thinner â†’ versicolor

wider â†’ virginica

ğŸŒ¸ Rule 3 â€” Large Petals
petal length > 5.15 â†’ virginica


Large petals â†’ always virginica

Correct real-world pattern.

ğŸŒ³ Why Petal Length Is Root Node

ID3 chooses split with:

maximum information gain


Petal length separates setosa perfectly â†’ highest gain â†’ chosen first.

This confirms your ID3 is working correctly





ğŸ”¹ Step 1 â€” Import Libraries
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

âœ… What this does

load_iris â†’ loads Iris dataset

DecisionTreeClassifier â†’ builds decision tree

criterion="entropy" later â†’ makes it ID3

plot_tree â†’ draws tree diagram

export_text â†’ prints rules as text

train_test_split â†’ splits data

metrics â†’ check accuracy

ğŸ”¹ Step 2 â€” Load Dataset
iris = load_iris()
X = iris.data
y = iris.target

âœ… Meaning

X = features (flower measurements)

y = class label (species)

Features:

sepal length
sepal width
petal length
petal width


Classes:

setosa, versicolor, virginica

ğŸ”¹ Step 3 â€” Save Names (for readable output)
feature_names = iris.feature_names
class_names = iris.target_names

âœ… Why needed

So tree diagram shows:

petal length <= 2.45


instead of:

feature_2 <= 2.45


Makes output understandable.

ğŸ”¹ Step 4 â€” Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.25,
    random_state=42
)

âœ… Meaning

Split dataset:

75% â†’ training

25% â†’ testing

Why?

ğŸ‘‰ Train tree on one part
ğŸ‘‰ Test performance on unseen data

random_state â†’ ensures same split every run (important for lab repeatability)

ğŸ”¹ Step 5 â€” Build ID3 Tree
model = DecisionTreeClassifier(
    criterion="entropy",
    max_depth=4,
    random_state=42
)

âœ… This is the core ID3 step
criterion="entropy"

Means:

Use entropy + information gain


That = ID3 algorithm

max_depth=4

Limits tree height.

Why?

Without limit â†’ huge tree

Hard to read

Overfitting risk

Teacher-friendly tree = readable tree.

ğŸ”¹ Step 6 â€” Train Model
model.fit(X_train, y_train)

âœ… What happens here

Tree learns rules like:

if petal length <= 2.45 â†’ setosa
else if petal width <= 1.75 â†’ versicolor
else â†’ virginica


This is ID3 rule building.

ğŸ”¹ Step 7 â€” Prediction
y_pred = model.predict(X_test)

âœ… Meaning

Model predicts species for test flowers.

ğŸ”¹ Step 8 â€” Accuracy Check
print(accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

âœ… Output tells

Accuracy %

Precision

Recall

F1 score

Shows model quality.

ğŸ”¹ Step 9 â€” Print Decision Rules (Very Important)
print(export_text(model, feature_names=feature_names))

âœ… This prints human-readable rules

Example:

petal length <= 2.45 â†’ setosa
petal width <= 1.75 â†’ versicolor


